{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to location of root directory\n",
    "root = \"/hdd/\"\n",
    "folders = [\"pringles_1k_1554610789151371002\",\n",
    "           \"black_decker_1k_1554675460418814897\",\n",
    "          \"lego_toy_1k_1554674193206299066\",\n",
    "          \"pringles_1k_1554677386797606945\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "class ourDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        \n",
    "        labelFileName = folder + \"/labels.txt\"\n",
    "        file = open(labelFileName,\"r\")\n",
    "        \n",
    "        lines = file.readlines()\n",
    "        self.count = len(lines)\n",
    "        self.examples = []\n",
    "        for line in lines:\n",
    "            array = line.split(\" \")\n",
    "            example = [array[0],array[1],\n",
    "                       int(array[2]),int(array[3]),\n",
    "                       int(array[4]),int(array[5])]\n",
    "            #print(example)\n",
    "            self.examples.append(example)\n",
    "            \n",
    "        self.image_folder = folder + \"/images/\"\n",
    "        self.transformations = \\\n",
    "            transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # stuff\n",
    "        \n",
    "        single_image_name = self.image_folder +self.examples[index][0]\n",
    "        img_as_img = Image.open(single_image_name)\n",
    "        img = self.transformations(img_as_img) \n",
    "        label = 1\n",
    "        \n",
    "        \n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count # of how many data(images?) you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "validation_split = .2\n",
    "random_seed= 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "datasets = []\n",
    "for f in folders:\n",
    "    folder = root + f\n",
    "    datasets.append(ourDataset(folder))\n",
    "dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
